{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, is_anomaly = generate_data(\n",
    "    n_features=6,\n",
    "    train_only=True,\n",
    "    random_state=1234\n",
    "    )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data, columns = ['x1','x2','x3','x4','x5', 'x6'])\n",
    "data['class'] = is_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,3)\n",
    "np.random.seed(1234)\n",
    "sns.scatterplot(\n",
    "    data = data, \n",
    "    x = 'x1', \n",
    "    y = 'x2', \n",
    "    hue = 'class'\n",
    ")\n",
    "\n",
    "plt.title(\"Generated Random Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train isolation forest\n",
    "clf = IsolationForest(max_samples=100, random_state=1234)\n",
    "clf.fit(data.drop('class', axis=1))\n",
    "predictions=clf.predict(data.drop('class', axis=1))\n",
    "anomaly_score = clf.decision_function(data.drop('class', axis=1))\n",
    "#map predictions 1 to 0, -1 to 1\n",
    "predictions[predictions == 1] = 0\n",
    "predictions[predictions == -1] = 1\n",
    "\n",
    "#add the predictions as a column to data\n",
    "data['predicted_class'] = predictions\n",
    "data['anomaly_score'] = anomaly_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data = data, \n",
    "    x = 'x1', \n",
    "    y = 'x2', \n",
    "    hue = 'class',\n",
    "    style = 'predicted_class'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot boxplot for anomaly score\n",
    "sns.boxplot(\n",
    "    data = data,\n",
    "    x = 'predicted_class',\n",
    "    y = 'anomaly_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_Forest(parameters):\n",
    "    model=IsolationForest(**parameters)\n",
    "    model.fit(data.drop('class', axis=1))\n",
    "    predictions=model.predict(data.drop('class', axis=1))\n",
    "    anomaly_score = model.decision_function(data.drop('class', axis=1))\n",
    "    #map predictions 1 to 0, -1 to 1\n",
    "    predictions[predictions == 1] = 0\n",
    "    predictions[predictions == -1] = 1\n",
    "\n",
    "    #add the predictions as a column to data\n",
    "    data['predicted_class'] = predictions\n",
    "    data['anomaly_score'] = anomaly_score\n",
    "\n",
    "    return predictions, anomaly_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a parameter grid for the isolation forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_samples': [50, 100, 200, 500],\n",
    "    'contamination': [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "#set a list to store the results\n",
    "results = []\n",
    "\n",
    "#loop through the parameter grid\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "\n",
    "    for max_samples in param_grid['max_samples']:\n",
    "\n",
    "        for contamination in param_grid['contamination']:\n",
    "\n",
    "            parameters = {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_samples': max_samples,\n",
    "                'contamination': contamination,\n",
    "                'random_state': 42\n",
    "            }\n",
    "\n",
    "            predictions, anomaly_score = isolation_Forest(parameters)\n",
    "\n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_samples': max_samples,\n",
    "                'contamination': contamination,\n",
    "                'precision_recall_curve': precision_recall_curve(data['class'], predictions),\n",
    "                'f1_score': f1_score(data['class'], predictions),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the results to a pandas dataframe\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "#find the best parameters\n",
    "best_parameters = results.loc[results['f1_score'].idxmax()]\n",
    "\n",
    "print(best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(results['f1_score'])\n",
    "plt.xlabel('Parameter Set')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score for Different Parameter Sets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class HashingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features_per_category=20, input_type=\"categorical\"):\n",
    "        self.n_features_per_category = n_features_per_category\n",
    "        self.input_type = input_type\n",
    "        self.encoders = []  # List to store encoders for remaining features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders.append(OneHotEncoder(sparse=False))  # Encoder for protocol_type (OHE)\n",
    "        self.encoders[0].fit(X[:, 0:1])  # Fit the encoder on the first column (protocol_type)\n",
    "\n",
    "        # Calculate total features needed for remaining features (excluding protocol_type)\n",
    "        remaining_features = X.shape[1] - 1\n",
    "        total_n_features = remaining_features * self.n_features_per_category\n",
    "\n",
    "        # Create and fit hashers for remaining features with adjusted n_features\n",
    "        for i in range(1, remaining_features):\n",
    "            self.encoders.append(FeatureHasher(n_features=total_n_features // remaining_features,\n",
    "                                                input_type=input_type))\n",
    "            self.encoders[i].fit(X[:, i:i+1])  # Fit each hasher on its corresponding column\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        protocol_encoded = self.encoders[0].transform(X[:, 0:1])\n",
    "        other_features_hashed = np.concatenate([encoder.transform(X[:, i:i+1])\n",
    "                                                for i, encoder in enumerate(self.encoders[1:])], axis=1)\n",
    "        return np.concatenate((protocol_encoded, other_features_hashed), axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
